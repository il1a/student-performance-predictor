{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Welcome to the Data Pipeline Master Notebook!\n",
    "\n",
    "This notebook consists of three main parts:\n",
    "1. **Data Scraping**\n",
    "2. **Data Preprocessing**\n",
    "3. **Data Splitting**\n",
    "\n",
    "For each of these parts, dedicated Python scripts have been created that are later used for deployment with Docker.\n",
    "\n",
    "> **WARNING:** These scripts must be executed strictly in the order listed above!\n"
   ],
   "id": "3827495d4a9a69eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "ce8239b279b003b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add the code directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "code_path = os.path.abspath(os.path.join('..'))\n",
    "if code_path not in sys.path:\n",
    "    sys.path.append(code_path)"
   ],
   "id": "edd46477c2e46043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the save_plot function from utils\n",
    "from utils.plot_saver import save_plot"
   ],
   "id": "8a8e217cbb341ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1: Data Scraping",
   "id": "f998916188b4d0c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Scrape the data from the .csv file saved on projects github repo",
   "id": "59ab5596f6dc70ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# URL of the raw CSV file\n",
    "url = 'https://raw.githubusercontent.com/il1a/student-performance-predictor/refs/heads/main/data/raw/original_data.csv'\n",
    "\n",
    "# Make an HTTP GET request to fetch the raw CSV content\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    csv_data = StringIO(response.text)\n",
    "    df = pd.read_csv(csv_data)\n",
    "    print(\"Data scraped successfully! Here's the DataFrame information:\")\n",
    "    df.info()\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
   ],
   "id": "708d2168699213b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Print some random data samples",
   "id": "7598f162ab0e0cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n10 random data samples:\")\n",
    "print(df.sample(10))"
   ],
   "id": "657ba1967ffce9c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Data Preprocessing",
   "id": "a60ef2903ceb4f85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Check the number of missing values",
   "id": "edcd10e71966bf5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_na = df.isna().sum().sum()\n",
    "print(\"\\nInitial total number of NA values in the dataset:\", total_na)"
   ],
   "id": "b1e03c8c33a592c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Impute missing values",
   "id": "4d06c0758def251b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#    - Numeric columns: fill with mean\n",
    "#    - Categorical columns: fill with mode\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numeric columns with mean\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with mode\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Verify that no missing values remain\n",
    "total_na_after_imputation = df.isna().sum().sum()\n",
    "print(\"\\nTotal number of NA values in the dataset after imputation:\", total_na_after_imputation)"
   ],
   "id": "847380729262c81d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Convert categorical columns to numeric",
   "id": "f6f1495d51a8b7f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Create dictionaries with mappings\n",
    "yes_no_mapping = {\n",
    "    'No': 0,\n",
    "    'Yes': 1\n",
    "}\n",
    "\n",
    "gender_mapping = {\n",
    "    'Female': 0,\n",
    "    'Male': 1\n",
    "}\n",
    "\n",
    "ordinal_mappings = {\n",
    "    'Low': 1,\n",
    "    'Medium': 2,\n",
    "    'High': 3\n",
    "}\n",
    "\n",
    "pos_neut_neg_mapping = {\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3\n",
    "}\n",
    "\n",
    "dist_mapping = {\n",
    "    'Near': 1,\n",
    "    'Moderate': 2,\n",
    "    'Far': 3\n",
    "}\n",
    "\n",
    "edu_mapping = {\n",
    "    'High School': 1,\n",
    "    'College': 2,\n",
    "    'Postgraduate': 3\n",
    "}\n",
    "\n",
    "# Apply mappings to categorical columns\n",
    "for col in ['Extracurricular_Activities', 'Internet_Access', 'Learning_Disabilities']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(yes_no_mapping)\n",
    "\n",
    "for col in ['Parental_Involvement', 'Access_to_Resources', 'Motivation_Level', 'Family_Income', 'Teacher_Quality']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(ordinal_mappings)\n",
    "\n",
    "if 'Gender' in df.columns:\n",
    "    df['Gender'] = df['Gender'].map(gender_mapping)\n",
    "\n",
    "if 'Peer_Influence' in df.columns:\n",
    "    df['Peer_Influence'] = df['Peer_Influence'].map(pos_neut_neg_mapping)\n",
    "\n",
    "if 'Distance_from_Home' in df.columns:\n",
    "    df['Distance_from_Home'] = df['Distance_from_Home'].map(dist_mapping)\n",
    "\n",
    "if 'Parental_Education_Level' in df.columns:\n",
    "    df['Parental_Education_Level'] = df['Parental_Education_Level'].map(edu_mapping)"
   ],
   "id": "d781712df973e08c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 Choose only relevant variables (X features) based on Pearson correlation with target (Exam_Score)",
   "id": "4b82135bf0fc5886"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_col = 'Exam_Score'\n",
    "\n",
    "# Ensure all columns are numeric now for the correlation matrix\n",
    "df_for_corr = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_for_corr.corr()\n",
    "\n",
    "# Look at correlation with target\n",
    "corr_with_target = corr_matrix[target_col].abs().sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with target (absolute values):\\n\", corr_with_target)\n",
    "\n",
    "# Keep top 5 correlated features + the target\n",
    "top_features = corr_with_target.index[0:6]\n",
    "print(\"\\nFinal top 5 features and target:\\n\", top_features)\n",
    "\n",
    "# Only leave the selected top 5 features in the dataset\n",
    "df_top = df_for_corr[top_features]\n",
    "\n",
    "# Separate X and y\n",
    "X = df_top.drop(columns=[target_col])\n",
    "y = df_top[target_col]"
   ],
   "id": "4a017f6fa674efe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 Visualise the correlation matrix using a heatmap",
   "id": "31006ac032b3f53d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the correlation matrix with top 5 features\n",
    "corr_matrix_top = df_top.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_top, annot=True, cmap='magma', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "\n",
    "# Save the current figure using the custom function\n",
    "save_plot(plt.gcf(), 'correlation_matrix.png')\n",
    "\n",
    "# Display the correlation matrix\n",
    "plt.show()"
   ],
   "id": "8e8a181dacee301e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Outlier detection and removal using IQR on the selected columns + target",
   "id": "a8a45dd7efe2d206"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define additional function for IQR outlier removal\n",
    "def remove_outliers_iqr(dataframe, columns, k=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers from the specified columns using the IQR method.\n",
    "    k=1.5 is the default multiplier.\n",
    "    \"\"\"\n",
    "    df_out = dataframe.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_out[col].quantile(0.25)\n",
    "        Q3 = df_out[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - k * IQR\n",
    "        upper_bound = Q3 + k * IQR\n",
    "\n",
    "        # Filter out rows outside the IQR bounds\n",
    "        df_out = df_out[(df_out[col] >= lower_bound) & (df_out[col] <= upper_bound)]\n",
    "    return df_out\n",
    "\n",
    "# Combine X and y temporarily for outlier removal\n",
    "df_xy = X.copy()\n",
    "df_xy[target_col] = y\n",
    "\n",
    "# Identify numeric columns for visualization\n",
    "numeric_cols = df_xy.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Box plots before outlier removal\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_xy[numeric_cols], orient='v')\n",
    "plt.title(\"Box plots Before Removing Outliers\")\n",
    "plt.tight_layout()\n",
    "save_plot(plt.gcf(), 'boxplots_outliers.png')\n",
    "plt.show()\n",
    "\n",
    "# Perform outlier removal using IQR function defined earlier\n",
    "df_xy_clean = remove_outliers_iqr(df_xy, numeric_cols, k=1.5)\n",
    "\n",
    "# Box plots after outlier removal\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_xy_clean[numeric_cols], orient='v')\n",
    "plt.title(\"Box plots After Removing Outliers\")\n",
    "plt.tight_layout()\n",
    "save_plot(plt.gcf(), 'boxplots_no_outliers.png')\n",
    "plt.show()\n",
    "\n",
    "# Update X and y after outlier removal\n",
    "X = df_xy_clean.drop(columns=[target_col])\n",
    "y = df_xy_clean[target_col]\n",
    "\n",
    "print(f\"\\nShape before outlier removal: {df_xy.shape}\")\n",
    "print(f\"Shape after outlier removal: {df_xy_clean.shape}\")"
   ],
   "id": "3e657490fde5d70b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 Check data distributions visually using histograms before normalization",
   "id": "de025c070dd9564f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "df_xy_clean.hist(bins=30, figsize=(12, 12))\n",
    "plt.tight_layout()\n",
    "save_plot(plt.gcf(), 'histograms_before_norm.png')\n",
    "plt.show()"
   ],
   "id": "f047728a91304fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.8 Print summary statistics before normalization",
   "id": "fc801dd483c90239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSummary statistics before normalization:\")\n",
    "print(df_xy_clean.describe())"
   ],
   "id": "a6c95243c7e644f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.9 Perform normalization using the StandardScaler",
   "id": "ddfa48581a8c0b95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Combine back X and y into one final dataset\n",
    "final_clean_df = X.copy()\n",
    "final_clean_df[target_col] = y.values"
   ],
   "id": "fb40cee7100ccde3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.10 Check data distributions visually using histograms after normalization",
   "id": "11fcbcf0e2f2a70d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "final_clean_df.hist(bins=30, figsize=(12, 12))\n",
    "plt.tight_layout()\n",
    "save_plot(plt.gcf(), 'histograms_after_norm.png')\n",
    "plt.show()"
   ],
   "id": "ec879405c00d98cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.11 Print summary statistics after normalization",
   "id": "74a6d500eebfb895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSummary statistics after normalization:\")\n",
    "print(final_clean_df.describe())"
   ],
   "id": "606c50273b968a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.12 Save the final clean dataset under data/processed",
   "id": "899edbb2dd6e51fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processed_data_dir = os.path.join('..', '..', 'data', 'processed')\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "joint_dataset_path = os.path.join(processed_data_dir, 'joint_data_collection.csv')\n",
    "final_clean_df.to_csv(joint_dataset_path, index=False)\n",
    "print(f\"\\nFinal clean dataset saved to {joint_dataset_path}\")"
   ],
   "id": "2849936d2b608b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3: Data Splitting",
   "id": "2f41744b263c8a99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Split the data (80% training, 20% test), save as CSV under data/processed",
   "id": "62c3818e4053ca3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "train_df[target_col] = y_train.values\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "test_df[target_col] = y_test.values\n",
    "\n",
    "train_path = os.path.join(processed_data_dir, 'training_data.csv')\n",
    "test_path = os.path.join(processed_data_dir, 'test_data.csv')\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Training data saved to {train_path}\")\n",
    "print(f\"Test data saved to {test_path}\")"
   ],
   "id": "942bc1a49f06f85e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Create activation data (single data entry from test set), save as CSV under data/processed",
   "id": "fcf98944e08da66a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "activation_df = test_df.sample(n=1, random_state=42)\n",
    "activation_path = os.path.join(processed_data_dir, 'activation_data.csv')\n",
    "activation_df.to_csv(activation_path, index=False)\n",
    "\n",
    "print(f\"Activation data (1 row) saved to {activation_path}\")"
   ],
   "id": "7599674ca28ea8ce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
